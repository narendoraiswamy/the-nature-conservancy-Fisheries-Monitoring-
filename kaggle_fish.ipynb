{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0,
  "cells": [
    {
      "cell_type": "markdown",
      "source": null,
      "execution_count": null,
      "outputs": [],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": "import numpy as np\nnp.random.seed(2016)\n\nimport os\nimport glob\nimport cv2\nimport datetime\nimport pandas as pd\nimport time\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom sklearn.cross_validation import KFold\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Flatten\nfrom keras.layers.convolutional import Convolution2D, MaxPooling2D, ZeroPadding2D\nfrom keras.optimizers import SGD\nfrom keras.callbacks import EarlyStopping\nfrom keras.utils import np_utils\nfrom sklearn.metrics import log_loss\nfrom keras import __version__ as keras_version\n\n\n\ndef get_im_cv2(path):\n    img = cv2.imread(path)\n    new = cv2.resize(img, (32, 32), cv2.INTER_LINEAR)\n    return new\n\n\ndef load_train():\n    X_train = []\n    X_train_id = []\n    y_train = []\n    start_time = time.time()\n\n    print('Read train images')\n    folders = ['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT']\n    for fld in folders:\n        index = folders.index(fld)\n        print('Load folder {} (Index: {})'.format(fld, index))\n        path = os.path.join('..', 'input', 'train', fld, '*.jpg')\n        files = glob.glob(path)\n        for fl in files:\n            flbase = os.path.basename(fl)\n            img = get_im_cv2(fl)\n            X_train.append(img)\n            X_train_id.append(flbase)\n            y_train.append(index)   #what does this do?\n\n    print('Read train data time: {} seconds'.format(round(time.time() - start_time, 2)))\n    return X_train, y_train, X_train_id\n\n\ndef load_test():\n    path = os.path.join('..', 'input', 'test_stg1', '*.jpg')\n    files = sorted(glob.glob(path))\n\n    X_test = []\n    X_test_id = []\n    for fl in files:\n        flbase = os.path.basename(fl)\n        img = get_im_cv2(fl)\n        X_test.append(img)\n        X_test_id.append(flbase)\n\n    return X_test, X_test_id\n\n\ndef create_submission(predictions, test_id, info):\n    result1 = pd.DataFrame(predictions, columns=['ALB', 'BET', 'DOL', 'LAG', 'NoF', 'OTHER', 'SHARK', 'YFT'])\n    result1.loc[:, 'image'] = pd.Series(test_id, index=result1.index)\n    now = datetime.datetime.now()\n    sub_file = 'submission_' + info + '_' + str(now.strftime(\"%Y-%m-%d-%H-%M\")) + '.csv'\n    result1.to_csv(sub_file, index=False)\n\n\ndef read_and_normalize_train_data():\n    train_data, train_target, train_id = load_train()\n\n    print('Convert to numpy...')\n    train_data = np.array(train_data, dtype=np.uint8)\n    train_target = np.array(train_target, dtype=np.uint8)\n\n    print('Reshape...')\n    train_data = train_data.transpose((0, 3, 1, 2))\n\n    print('Convert to float...')\n    train_data = train_data.astype('float32')\n    train_data = train_data / 255\n    train_target = np_utils.to_categorical(train_target, 8)\n\n    print('Train shape:', train_data.shape)\n    print(train_data.shape[0], 'train samples')\n    return train_data, train_target, train_id\n\n\ndef read_and_normalize_test_data():\n    start_time = time.time()\n    test_data, test_id = load_test()\n\n    test_data = np.array(test_data, dtype=np.uint8)\n    test_data = test_data.transpose((0, 3, 1, 2))\n\n    test_data = test_data.astype('float32')\n    test_data = test_data / 255\n\n    print('Test shape:', test_data.shape)\n    print(test_data.shape[0], 'test samples')\n    print('Read and process test data time: {} seconds'.format(round(time.time() - start_time, 2)))\n    return test_data, test_id\n\n\ndef dict_to_list(d):\n    ret = []\n    for i in d.items():\n        ret.append(i[1])\n    return ret\n\n\ndef merge_several_folds_mean(data, nfolds):\n    a = np.array(data[0])\n    for i in range(1, nfolds):\n        a += np.array(data[i])\n    a /= nfolds\n    return a.tolist()\n\n\ndef create_model():\n    model = Sequential()\n    model.add(ZeroPadding2D((1, 1), input_shape=(3, 32, 32), dim_ordering='th'))\n    model.add(Convolution2D(4, 3, 3, activation='relu', dim_ordering='th'))\n    model.add(ZeroPadding2D((1, 1), dim_ordering='th'))\n    model.add(Convolution2D(4, 3, 3, activation='relu', dim_ordering='th'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), dim_ordering='th'))\n\n    model.add(ZeroPadding2D((1, 1), dim_ordering='th'))\n    model.add(Convolution2D(8, 3, 3, activation='relu', dim_ordering='th'))\n    model.add(ZeroPadding2D((1, 1), dim_ordering='th'))\n    model.add(Convolution2D(8, 3, 3, activation='relu', dim_ordering='th'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), dim_ordering='th'))\n    \n    model.add(ZeroPadding2D((1, 1), dim_ordering='th'))   \n    model.add(Convolution2D(16, 3, 3, activation='relu', dim_ordering='th'))\n    model.add(ZeroPadding2D((1, 1), dim_ordering='th'))   \n    model.add(Convolution2D(16, 3, 3, activation='relu', dim_ordering='th'))\n    model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), dim_ordering='th'))\n    model.add(Dropout(0.2))\n    \n\n    model.add(Flatten())\n    model.add(Dense(32, activation='relu'))\n    model.add(Dropout(0.5))\n    model.add(Dense(32, activation='relu'))\n    model.add(Dropout(2.5)) #changed this from 0.5\n    model.add(Dense(8, activation='softmax'))\n\n    sgd = SGD(lr=1e-2, decay=1e-6, momentum=0.9, nesterov=True)\n    model.compile(optimizer=sgd, loss='categorical_crossentropy')\n\n    return model\n\n\ndef get_validation_predictions(train_data, predictions_valid):\n    pv = []\n    for i in range(len(train_data)):\n        pv.append(predictions_valid[i])\n    return pv\n\n\ndef run_cross_validation_create_models(nfolds=10):\n    # input image dimensions\n    batch_size = 32\n    nb_epoch = 10\n    random_state = 51\n\n    train_data, train_target, train_id = read_and_normalize_train_data()\n\n    yfull_train = dict()\n    kf = KFold(len(train_id), n_folds=nfolds, shuffle=True, random_state=random_state)\n    num_fold = 0\n    sum_score = 0\n    models = []\n    for train_index, test_index in kf:\n        model = create_model()\n        X_train = train_data[train_index]\n        Y_train = train_target[train_index]\n        X_valid = train_data[test_index]\n        Y_valid = train_target[test_index]\n\n        num_fold += 1\n        print('Start KFold number {} from {}'.format(num_fold, nfolds))\n        print('Split train: ', len(X_train), len(Y_train))\n        print('Split valid: ', len(X_valid), len(Y_valid))\n\n        callbacks = [\n            EarlyStopping(monitor='val_loss', patience=3, verbose=0),\n        ]\n        model.fit(X_train, Y_train, batch_size=batch_size, nb_epoch=nb_epoch,\n              shuffle=True, verbose=2, validation_data=(X_valid, Y_valid),\n              callbacks=callbacks)\n\n        predictions_valid = model.predict(X_valid.astype('float32'), batch_size=batch_size, verbose=2)\n        score = log_loss(Y_valid, predictions_valid)\n        print('Score log_loss: ', score)\n        sum_score += score*len(test_index)\n\n        # Store valid predictions\n        for i in range(len(test_index)):\n            yfull_train[test_index[i]] = predictions_valid[i]\n\n        models.append(model)\n\n    score = sum_score/len(train_data)\n    print(\"Log_loss train independent avg: \", score)\n\n    info_string = 'loss_' + str(score) + '_folds_' + str(nfolds) + '_ep_' + str(nb_epoch)\n    return info_string, models\n\n\ndef run_cross_validation_process_test(info_string, models):\n    batch_size = 16\n    num_fold = 0\n    yfull_test = []\n    test_id = []\n    nfolds = len(models)\n\n    for i in range(nfolds):\n        model = models[i]\n        num_fold += 1\n        print('Start KFold number {} from {}'.format(num_fold, nfolds))\n        test_data, test_id = read_and_normalize_test_data()\n        test_prediction = model.predict(test_data, batch_size=batch_size, verbose=2)\n        yfull_test.append(test_prediction)\n\n    test_res = merge_several_folds_mean(yfull_test, nfolds)\n    info_string = 'loss_' + info_string \\\n                + '_folds_' + str(nfolds)\n    create_submission(test_res, test_id, info_string)\n\n\nif __name__ == '__main__':\n    print('Keras version: {}'.format(keras_version))\n    num_folds = 3\n    info_string, models = run_cross_validation_create_models(num_folds)\n    run_cross_validation_process_test(info_string, models)\n    ",
      "execution_count": 1,
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": "/opt/conda/lib/python3.6/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n  \"This module will be removed in 0.20.\", DeprecationWarning)\nUsing TensorFlow backend.\n"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": "Keras version: 2.0.0\nRead train images\nLoad folder ALB (Index: 0)\nLoad folder BET (Index: 1)\nLoad folder DOL (Index: 2)\nLoad folder LAG (Index: 3)\nLoad folder NoF (Index: 4)\nLoad folder OTHER (Index: 5)\nLoad folder SHARK (Index: 6)\nLoad folder YFT (Index: 7)\nRead train data time: 33.54 seconds\nConvert to numpy...\nReshape...\nConvert to float...\nTrain shape: (3777, 3, 32, 32)\n3777 train samples\nStart KFold number 1 from 3\nSplit train:  2518 2518\nSplit valid:  1259 1259\nTrain on 2518 samples, validate on 1259 samples\nEpoch 1/10\n12s - loss: 1.7334 - val_loss: 1.6214\nEpoch 2/10\n14s - loss: 1.6173 - val_loss: 1.6190\nEpoch 3/10\n14s - loss: 1.6147 - val_loss: 1.6148\nEpoch 4/10\n14s - loss: 1.6128 - val_loss: 1.6095\nEpoch 5/10\n13s - loss: 1.6024 - val_loss: 1.5890\nEpoch 6/10\n12s - loss: 1.5751 - val_loss: 1.5470\nEpoch 7/10\n12s - loss: 1.5355 - val_loss: 1.4761\nEpoch 8/10\n13s - loss: 1.4914 - val_loss: 1.4126\nEpoch 9/10\n13s - loss: 1.4460 - val_loss: 1.3353\nEpoch 10/10\n13s - loss: 1.4140 - val_loss: 1.3038\nScore log_loss:  1.30382976469\nStart KFold number 2 from 3\nSplit train:  2518 2518\nSplit valid:  1259 1259\nTrain on 2518 samples, validate on 1259 samples\nEpoch 1/10\n13s - loss: 1.6879 - val_loss: 1.6266\nEpoch 2/10\n13s - loss: 1.6157 - val_loss: 1.6171\nEpoch 3/10\n13s - loss: 1.6020 - val_loss: 1.5882\nEpoch 4/10\n13s - loss: 1.5661 - val_loss: 1.5086\nEpoch 5/10\n13s - loss: 1.5063 - val_loss: 1.4724\nEpoch 6/10\n13s - loss: 1.4572 - val_loss: 1.3390\nEpoch 7/10\n13s - loss: 1.3808 - val_loss: 1.2646\nEpoch 8/10\n13s - loss: 1.3314 - val_loss: 1.2108\nEpoch 9/10\n13s - loss: 1.2863 - val_loss: 1.2014\nEpoch 10/10\n13s - loss: 1.2355 - val_loss: 1.1226\nScore log_loss:  1.12263423919\nStart KFold number 3 from 3\nSplit train:  2518 2518\nSplit valid:  1259 1259\nTrain on 2518 samples, validate on 1259 samples\nEpoch 1/10\n13s - loss: 1.7070 - val_loss: 1.6243\nEpoch 2/10\n13s - loss: 1.6190 - val_loss: 1.5664\nEpoch 3/10\n13s - loss: 1.5869 - val_loss: 1.5392\nEpoch 4/10\n13s - loss: 1.5566 - val_loss: 1.4927\nEpoch 5/10\n13s - loss: 1.5074 - val_loss: 1.4166\nEpoch 6/10\n13s - loss: 1.4654 - val_loss: 1.4206\nEpoch 7/10\n13s - loss: 1.4037 - val_loss: 1.2961\nEpoch 8/10\n13s - loss: 1.3364 - val_loss: 1.2500\nEpoch 9/10\n13s - loss: 1.2972 - val_loss: 1.1835\nEpoch 10/10\n13s - loss: 1.2297 - val_loss: 1.0552\nScore log_loss:  1.05518732303\nLog_loss train independent avg:  1.1605504423\nStart KFold number 1 from 3\nTest shape: (1000, 3, 32, 32)\n1000 test samples\nRead and process test data time: 8.57 seconds\nStart KFold number 2 from 3\nTest shape: (1000, 3, 32, 32)\n1000 test samples\nRead and process test data time: 8.65 seconds\nStart KFold number 3 from 3\nTest shape: (1000, 3, 32, 32)\n1000 test samples\nRead and process test data time: 8.98 seconds\n"
        }
      ],
      "metadata": {}
    },
    {
      "cell_type": "code",
      "source": null,
      "execution_count": 2,
      "outputs": [],
      "metadata": {}
    }
  ]
}